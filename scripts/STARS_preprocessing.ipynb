{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arTChmebnEgd",
        "outputId": "b16ebadf-6f89-444c-f130-17cdaa466256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksssV7-KnCYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89dcde85-3c46-4880-8caf-5afbb2befde0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/OGLE3_clean/ogle3-acep.zip',\n",
              " '/content/OGLE3_clean/ogle3-acv.zip',\n",
              " '/content/OGLE3_clean/ogle3-cep.zip',\n",
              " '/content/OGLE3_clean/ogle3-dn.zip',\n",
              " '/content/OGLE3_clean/ogle3-dpv.zip',\n",
              " '/content/OGLE3_clean/ogle3-dsct.zip',\n",
              " '/content/OGLE3_clean/ogle3-ecl.zip',\n",
              " '/content/OGLE3_clean/ogle3-lpv.zip',\n",
              " '/content/OGLE3_clean/ogle3-rcb.zip',\n",
              " '/content/OGLE3_clean/ogle3-rrlyr.zip',\n",
              " '/content/OGLE3_clean/ogle3-t2cep.zip',\n",
              " '/content/OGLE3_clean/ogle3-wd.zip',\n",
              " '/content/OGLE3_clean/ogle3-yso.zip']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/drive/folders/1L_6hJY0ye2VbSFhq2B1Rt34BbWj89WEQ\"\n",
        "\n",
        "\n",
        "\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import shutil"
      ],
      "metadata": {
        "id": "dO25TxFKO9Za"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "BIIBJML9Zrdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_zip_files(directory):\n",
        "    \"\"\"\n",
        "    Find zip files within a directory.\n",
        "\n",
        "    Args:\n",
        "    - directory (str): The directory to search for zip files.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of paths to zip files found in the directory.\n",
        "    \"\"\"\n",
        "    zip_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".zip\"):\n",
        "                zip_files.append(os.path.join(root, file))\n",
        "    return zip_files"
      ],
      "metadata": {
        "id": "wpWqR59mOEzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = '/content/OGLE3_clean'"
      ],
      "metadata": {
        "id": "NbqeQ9x6PmWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_files = find_zip_files(root_folder)\n",
        "print(zip_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umHPjwKFQoHy",
        "outputId": "963f3d97-bdef-45e2-90fb-bb40cf9bbfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/OGLE3_clean/ogle3-acv.zip', '/content/OGLE3_clean/ogle3-t2cep.zip', '/content/OGLE3_clean/ogle3-yso.zip', '/content/OGLE3_clean/ogle3-rcb.zip', '/content/OGLE3_clean/ogle3-ecl.zip', '/content/OGLE3_clean/ogle3-acep.zip', '/content/OGLE3_clean/ogle3-rrlyr.zip', '/content/OGLE3_clean/ogle3-wd.zip', '/content/OGLE3_clean/ogle3-dn.zip', '/content/OGLE3_clean/ogle3-dsct.zip', '/content/OGLE3_clean/ogle3-dpv.zip', '/content/OGLE3_clean/ogle3-cep.zip', '/content/OGLE3_clean/ogle3-lpv.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for zip_file in zip_files:\n",
        "    #!unzip -q \"$zip_file\" -d \"{os.path.splitext(zip_file)[0]}\"\n",
        "    !unzip -q \"$zip_file\" -d \"{os.path.dirname(os.path.splitext(zip_file)[0])}\"\n",
        "    print(\"extraction done for \"+ zip_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_5g2L7RPuFK",
        "outputId": "fbdc519f-5335-4c80-b837-15e1e1eabf83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extraction done for /content/OGLE3_clean/ogle3-acv.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-t2cep.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-yso.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-rcb.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-ecl.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-acep.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-rrlyr.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-wd.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-dn.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-dsct.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-dpv.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-cep.zip\n",
            "extraction done for /content/OGLE3_clean/ogle3-lpv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for zip_file in zip_files:\n",
        "  !rm -r \"$zip_file\"\n",
        "  print(\"delete done for \"+ zip_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oaY1cB4BQxj",
        "outputId": "ae543bac-62a6-4f8c-d211-099b87f35231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delete done for /content/OGLE3_clean/ogle3-acv.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-t2cep.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-yso.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-rcb.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-ecl.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-acep.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-rrlyr.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-wd.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-dn.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-dsct.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-dpv.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-cep.zip\n",
            "delete done for /content/OGLE3_clean/ogle3-lpv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/OGLE3_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uUbbhDpOMqE",
        "outputId": "5fdd4027-670f-4038-86c4-68276a75094d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acep  acv  cep\tdn  dpv  dsct  ecl  lpv  rcb  rrlyr  t2cep  wd\tyso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##augmentation"
      ],
      "metadata": {
        "id": "x9_ltsXi99EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_folder_counts(root_folder):\n",
        "    for class_folder in sorted(os.listdir(root_folder)):  # Sorted for consistent order\n",
        "        class_folder_path = os.path.join(root_folder, class_folder)\n",
        "        if os.path.isdir(class_folder_path):  # Ensure it's a folder\n",
        "            num_files = len([f for f in os.listdir(class_folder_path) if f.endswith('.dat')])\n",
        "            print(f\"{class_folder}: {num_files} files\")"
      ],
      "metadata": {
        "id": "B-NtJTnJDhz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print counts before augmentation\n",
        "print(\"Before augmentation:\")\n",
        "print_folder_counts(root_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3jI2kc_Dtqf",
        "outputId": "7d2877e7-d64c-4c1b-d5f0-fbb828d7d56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before augmentation:\n",
            "acep: 83 files\n",
            "acv: 5 files\n",
            "cep: 8026 files\n",
            "dn: 40 files\n",
            "dpv: 137 files\n",
            "dsct: 2859 files\n",
            "ecl: 43859 files\n",
            "lpv: 343816 files\n",
            "rcb: 23 files\n",
            "rrlyr: 44262 files\n",
            "t2cep: 603 files\n",
            "wd: 1 files\n",
            "yso: 3 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_light_curves(data_path, output_path):\n",
        "    # Read the original data\n",
        "    data = pd.read_csv(data_path, sep=\"\\s+\", header=None)\n",
        "    data.columns = ['Time', 'Magnitude', 'Err']\n",
        "\n",
        "    # Augment the data and save as supplementary .dat files\n",
        "    step = random.randint(1, 2)\n",
        "    burning = random.randint(0, 5)\n",
        "\n",
        "    augmented_data = np.roll(data.values, step, axis=0)[burning:]\n",
        "    np.savetxt(output_path, augmented_data, delimiter='\\t', fmt='%f')\n"
      ],
      "metadata": {
        "id": "fKjxrRV--pTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_class_folders(root_folder, n_max=8000, max_replications=5):\n",
        "    for class_folder in tqdm(os.listdir(root_folder)) :#, desc= f\"working on {class_folder}\", end = \"\"):\n",
        "        class_folder_path = os.path.join(root_folder, class_folder)\n",
        "\n",
        "        dat_files = [f for f in os.listdir(class_folder_path) if f.endswith('.dat')]\n",
        "        num_files = len(dat_files)\n",
        "        num_augmented_needed = max(0, n_max - num_files)\n",
        "\n",
        "        if num_augmented_needed > 0:\n",
        "            replication_count = {file: 0 for file in dat_files}\n",
        "            while num_augmented_needed > 0 and any(replication_count[file] < max_replications for file in dat_files):\n",
        "                for dat_file in random.sample(dat_files, len(dat_files)):\n",
        "                    if num_augmented_needed <= 0 or replication_count[dat_file] >= max_replications:\n",
        "                        continue\n",
        "\n",
        "                    data_path = os.path.join(class_folder_path, dat_file)\n",
        "                    output_file = os.path.join(class_folder_path, f\"{os.path.splitext(dat_file)[0]}_augmented_{replication_count[dat_file]}.dat\")\n",
        "                    augment_light_curves(data_path, output_file)\n",
        "\n",
        "                    replication_count[dat_file] += 1\n",
        "                    num_augmented_needed -= 1\n",
        "        #print()\n",
        "    print(f\"Finished augmenting folder\" )#end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oxzzCVKY-9RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment_class_folders(root_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY-ygymW_PQz",
        "outputId": "cf5fc647-c64e-46ed-e966-04a441d7c4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:52<00:00,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished augmenting folder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print counts before augmentation\n",
        "print(\"After augmentation:\")\n",
        "print_folder_counts(root_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3thAHRj2Dx7L",
        "outputId": "fb6f7e4f-745f-4beb-8ecb-042353de5521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After augmentation:\n",
            "acep: 498 files\n",
            "acv: 30 files\n",
            "cep: 8026 files\n",
            "dn: 240 files\n",
            "dpv: 822 files\n",
            "dsct: 8000 files\n",
            "ecl: 43859 files\n",
            "lpv: 343816 files\n",
            "rcb: 138 files\n",
            "rrlyr: 44262 files\n",
            "t2cep: 3618 files\n",
            "wd: 6 files\n",
            "yso: 18 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preprocessing"
      ],
      "metadata": {
        "id": "IcdkkalxODUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -r /content/OGLE3"
      ],
      "metadata": {
        "id": "3dCGkyrSP0Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess light curves\n",
        "def preprocess_light_curves(data_path):\n",
        "    # Read data from fixed-width file\n",
        "    #\"\"\"\n",
        "    data = pd.read_csv(data_path, sep = \"\\s+\", header=None).reset_index(drop=True)\n",
        "    data.columns = ['Time', 'Magnitude', 'Err']\n",
        "    data = data.astype('float64')\n",
        "    data = data[data.Time > 0]\n",
        "    data.sort_values(by=['Time'])\n",
        "    data.drop_duplicates(subset='Time', keep='first')\n",
        "\n",
        "\n",
        "\n",
        "    # Compute differences between consecutive observations for time and magnitude\n",
        "    # Note: diff() leaves the first element as NaN, which we replace with zeros\n",
        "    data['Time_diff'] = data['Time'].diff().fillna(0)\n",
        "    data['Magnitude_diff'] = data['Magnitude'].diff().fillna(0)\n",
        "\n",
        "    # Prepare for padding if necessary, targeting a minimum of 500 observations\n",
        "    min_observations = 500\n",
        "    padding_needed = max(0, min_observations - len(data))\n",
        "\n",
        "    # If padding is needed, extend the dataframe with rows of zeros\n",
        "    if padding_needed > 0:\n",
        "        padding_df = pd.DataFrame({\n",
        "            'Time': [0] * padding_needed,\n",
        "            'Magnitude': [0] * padding_needed,\n",
        "            'Err': [0] * padding_needed,\n",
        "            'Time_diff': [0] * padding_needed,\n",
        "            'Magnitude_diff': [0] * padding_needed\n",
        "        })\n",
        "        data = pd.concat([data, padding_df], ignore_index=True)\n",
        "\n",
        "    # After padding, if the dataset is larger than the target size, truncate it to the target size\n",
        "    data = data.head(min_observations)\n",
        "\n",
        "    # Since we're only interested in Time_diff and Magnitude_diff for the network input,\n",
        "    # we reshape our data accordingly to a shape of 1 × 2 × N (1 light curve, 2 features, N observations)\n",
        "    matrix_representation = np.stack([data['Time_diff'].values, data['Magnitude_diff'].values], axis=0).reshape(1, 2, -1)\n",
        "\n",
        "    return matrix_representation"
      ],
      "metadata": {
        "id": "NwBp9-XjPEjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_class_folders(root_folder, output_folder, preprocess_func):\n",
        "    \"\"\"\n",
        "    For each .dat file in the class folders, apply a preprocessing function and save the output as a CSV file.\n",
        "    Each light curve is saved to its own CSV file in a specified output directory.\n",
        "\n",
        "    Args:\n",
        "    - root_folder (str): Path to the directory containing class folders with .dat files.\n",
        "    - output_folder (str): Path to the directory where preprocessed folders and files will be stored.\n",
        "    - preprocess_func (function): The preprocessing function to be applied to each .dat file.\n",
        "    \"\"\"\n",
        "    # Iterate over each class folder in the root directory\n",
        "    for class_folder in os.listdir(root_folder):\n",
        "        class_folder_path = os.path.join(root_folder, class_folder)\n",
        "\n",
        "        # Only process directories\n",
        "        if os.path.isdir(class_folder_path):\n",
        "            # Create a new directory for preprocessed files within the specified output folder\n",
        "            preprocessed_folder_path = os.path.join(output_folder, f\"{class_folder}_preprocessed\")\n",
        "            if not os.path.exists(preprocessed_folder_path):\n",
        "                os.makedirs(preprocessed_folder_path)\n",
        "\n",
        "            all_dat_files = [f for f in os.listdir(class_folder_path) if f.endswith('.dat')]\n",
        "\n",
        "            # If more than 8000 .dat files exist, randomly select 8000 without repetition\n",
        "            if len(all_dat_files) > 8000:\n",
        "                selected_files = random.sample(all_dat_files, 8000)\n",
        "            else:\n",
        "                selected_files = all_dat_files\n",
        "\n",
        "            # Process each selected .dat file within the class folder\n",
        "            for dat_file in tqdm(selected_files, desc=f\"Processing {class_folder}\"):\n",
        "                dat_file_path = os.path.join(class_folder_path, dat_file)\n",
        "                preprocessed_data = preprocess_func(dat_file_path)\n",
        "\n",
        "                # Flatten the preprocessed data for saving\n",
        "                preprocessed_data_flat = preprocessed_data.reshape(2, -1).T  # Reshape to Nx2 for saving\n",
        "\n",
        "                # Define the new filename and path for the preprocessed file\n",
        "                preprocessed_file_name = f\"{os.path.splitext(dat_file)[0]}_preprocessed.csv\"\n",
        "                preprocessed_file_path = os.path.join(preprocessed_folder_path, preprocessed_file_name)\n",
        "\n",
        "                # Save the preprocessed data to the new file\n",
        "                np.savetxt(preprocessed_file_path, preprocessed_data_flat, delimiter=',', header='time_diff,mag_diff', comments='')\n",
        "\n",
        "            print(f\"Completed processing for class: {class_folder}.\")\n",
        "    print(\"Preprocessing complete.\")\n"
      ],
      "metadata": {
        "id": "YhVETWDsriMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = '/content/drive/MyDrive/ADV_ML/OGLE3_clean_preprocessed'"
      ],
      "metadata": {
        "id": "O9Lg_t-cgrH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_class_folders(root_folder, output_folder, preprocess_light_curves)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EuPLB0cr87S",
        "outputId": "0322c14b-d738-478f-e497-7c1b2e785127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dpv: 100%|██████████| 822/822 [00:20<00:00, 40.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: dpv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing acv: 100%|██████████| 30/30 [00:00<00:00, 43.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: acv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing lpv: 100%|██████████| 8000/8000 [03:29<00:00, 38.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: lpv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing yso: 100%|██████████| 18/18 [00:00<00:00, 39.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: yso.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing acep: 100%|██████████| 498/498 [00:12<00:00, 40.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: acep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing wd: 100%|██████████| 6/6 [00:00<00:00, 30.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: wd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dn: 100%|██████████| 240/240 [00:06<00:00, 40.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: dn.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cep: 100%|██████████| 8000/8000 [03:33<00:00, 37.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: cep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing ecl: 100%|██████████| 8000/8000 [03:48<00:00, 35.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: ecl.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dsct: 100%|██████████| 8000/8000 [03:29<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: dsct.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rcb: 100%|██████████| 138/138 [00:03<00:00, 42.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: rcb.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rrlyr: 100%|██████████| 8000/8000 [03:39<00:00, 36.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: rrlyr.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing t2cep: 100%|██████████| 3618/3618 [01:52<00:00, 32.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for class: t2cep.\n",
            "Preprocessing complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}